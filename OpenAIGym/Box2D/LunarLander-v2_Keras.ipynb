{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd6a49d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Create environment\n",
    "env_name = 'LunarLander-v2'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "def eval_env_random_actions(env, episodes = 10, render = True):\n",
    "    all_rewards = []\n",
    "    for episode in range(1, episodes):\n",
    "        env.reset() # Restart the agent at the beginning\n",
    "        done = False # If the agent has completed the level\n",
    "        score = 0 # Called score not return cause it's python\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            random_action = env.action_space.sample() # Do random actions\n",
    "            state, reward, done, info = env.step(random_action) \n",
    "            score += reward\n",
    "        print('Episode: {}\\t\\tScore: {}'.format(episode, score))\n",
    "        all_rewards.append(score)  \n",
    "    print(\"\\nMean reward:\", np.mean(all_rewards))\n",
    "    env.close()\n",
    "    \n",
    "# eval_env_random_actions(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1047fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================\n",
    "#\n",
    "#   File name   : LunarLander-v2_PPO.py\n",
    "#   Author      : PyLessons\n",
    "#   Created date: 2020-10-10\n",
    "#   Website     : https://pylessons.com/\n",
    "#   GitHub      : https://github.com/pythonlessons/Reinforcement_Learning\n",
    "#   Description : LunarLander-v2 PPO discrete agent\n",
    "#   TensorFlow  : 2.3.1\n",
    "#\n",
    "#================================================================\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1' # -1:cpu, 0:first gpu\n",
    "import random\n",
    "import gym\n",
    "import pylab\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c4836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.config.experimental_run_functions_eagerly(True) # used for debuging and development\n",
    "tf.compat.v1.disable_eager_execution() # usually using this for fastest performance\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras import backend\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05b33c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread, Lock\n",
    "from multiprocessing import Process, Pipe\n",
    "import time\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    print(f'GPUs {gpus}')\n",
    "    try: tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a30cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "        self.action_space = action_space\n",
    "\n",
    "        X = Dense(512, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X_input)\n",
    "        X = Dense(256, activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        X = Dense(64,  activation=\"relu\", kernel_initializer=tf.random_normal_initializer(stddev=0.01))(X)\n",
    "        output = Dense(self.action_space, activation=\"softmax\")(X)\n",
    "\n",
    "        self.Actor = Model(inputs = X_input, outputs = output)\n",
    "        self.Actor.compile(loss=self.ppo_loss, optimizer=optimizer(lr=lr))\n",
    "\n",
    "    def ppo_loss(self, y_true, y_pred):\n",
    "        # Defined in https://arxiv.org/abs/1707.06347\n",
    "        advantages, prediction_picks, actions = y_true[:, :1], y_true[:, 1:1+self.action_space], y_true[:, 1+self.action_space:]\n",
    "        LOSS_CLIPPING = 0.2\n",
    "        ENTROPY_LOSS = 0.001\n",
    "        \n",
    "        prob = actions * y_pred\n",
    "        old_prob = actions * prediction_picks\n",
    "\n",
    "        prob = backend.clip(prob, 1e-10, 1.0)\n",
    "        old_prob = backend.clip(old_prob, 1e-10, 1.0)\n",
    "\n",
    "        ratio = backend.exp(backend.log(prob) - backend.log(old_prob))\n",
    "        \n",
    "        p1 = ratio * advantages\n",
    "        p2 = backend.clip(ratio, min_value=1 - LOSS_CLIPPING, max_value=1 + LOSS_CLIPPING) * advantages\n",
    "\n",
    "        actor_loss = -backend.mean(backend.minimum(p1, p2))\n",
    "\n",
    "        entropy = -(y_pred * backend.log(y_pred + 1e-10))\n",
    "        entropy = ENTROPY_LOSS * backend.mean(entropy)\n",
    "        \n",
    "        total_loss = actor_loss - entropy\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Actor.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc9c76f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(Process):\n",
    "    def __init__(self, env_idx, child_conn, env_name, state_size, action_size, visualize=False):\n",
    "        super(Environment, self).__init__()\n",
    "        self.env = gym.make(env_name)\n",
    "        self.is_render = False #visualize\n",
    "        self.env_idx = env_idx\n",
    "        self.child_conn = child_conn\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def run(self):\n",
    "        super(Environment, self).run()\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        self.child_conn.send(state)\n",
    "        while True:\n",
    "            action = self.child_conn.recv()\n",
    "            if self.is_render and self.env_idx == 0:\n",
    "                self.env.render()\n",
    "\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "                state = np.reshape(state, [1, self.state_size])\n",
    "\n",
    "            self.child_conn.send([state, reward, done, info])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "224584ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_Model:\n",
    "    def __init__(self, input_shape, action_space, lr, optimizer):\n",
    "        X_input = Input(input_shape)\n",
    "        old_values = Input(shape=(1,))\n",
    "\n",
    "        V = Dense(512, activation=\"relu\", kernel_initializer='he_uniform')(X_input)\n",
    "        V = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "        V = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(V)\n",
    "        value = Dense(1, activation=None)(V)\n",
    "\n",
    "        self.Critic = Model(inputs=[X_input, old_values], outputs = value)\n",
    "        self.Critic.compile(loss=[self.critic_PPO2_loss(old_values)], optimizer=optimizer(lr=lr))\n",
    "\n",
    "    def critic_PPO2_loss(self, values):\n",
    "        def loss(y_true, y_pred):\n",
    "            LOSS_CLIPPING = 0.2\n",
    "            clipped_value_loss = values + backend.clip(y_pred - values, -LOSS_CLIPPING, LOSS_CLIPPING)\n",
    "            v_loss1 = (y_true - clipped_value_loss) ** 2\n",
    "            v_loss2 = (y_true - y_pred) ** 2\n",
    "            \n",
    "            value_loss = 0.5 * backend.mean(backend.maximum(v_loss1, v_loss2))\n",
    "            #value_loss = backend.mean((y_true - y_pred) ** 2) # standard PPO loss\n",
    "            return value_loss\n",
    "        return loss\n",
    "\n",
    "    def predict(self, state):\n",
    "        return self.Critic.predict([state, np.zeros((state.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a1f9a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1296x648 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class PPOAgent:\n",
    "    # PPO Main Optimization Algorithm\n",
    "    def __init__(self, env_name):\n",
    "        # Initialization\n",
    "        # Environment and PPO parameters\n",
    "        self.env_name = env_name       \n",
    "        self.env = gym.make(env_name)\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.state_size = self.env.observation_space.shape\n",
    "        self.EPISODES = 10_0#00 # total episodes to train through all environments\n",
    "        self.episode = 0 # used to track the episodes total count of episodes played through all thread environments\n",
    "        self.max_average = 0 # when average score is above 0 model will be saved\n",
    "        self.lr = 0.00025\n",
    "        self.epochs = 10 # training epochs\n",
    "        self.shuffle=False\n",
    "        self.Training_batch = 1000\n",
    "        #self.optimizer = RMSprop\n",
    "        self.optimizer = Adam\n",
    "\n",
    "        self.replay_count = 0\n",
    "        self.writer = SummaryWriter(comment=\"_\"+self.env_name+\"_\"+self.optimizer.__name__+\"_\"+str(self.lr))\n",
    "        \n",
    "        # Instantiate plot memory\n",
    "        self.scores_, self.episodes_, self.average_ = [], [], [] # used in matplotlib plots\n",
    "\n",
    "        # Create Actor-Critic network models\n",
    "        self.Actor = Actor_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n",
    "        self.Critic = Critic_Model(input_shape=self.state_size, action_space = self.action_size, lr=self.lr, optimizer = self.optimizer)\n",
    "        \n",
    "        self.Actor_name = f\"{self.env_name}_PPO_Actor.h5\"\n",
    "        self.Critic_name = f\"{self.env_name}_PPO_Critic.h5\"\n",
    "\n",
    "        \n",
    "    def act(self, state):\n",
    "        \"\"\" example:\n",
    "        pred = np.array([0.05, 0.85, 0.1])\n",
    "        action_size = 3\n",
    "        np.random.choice(a, p=pred)\n",
    "        result>>> 1, because it have the highest probability to be taken\n",
    "        \"\"\"\n",
    "        # Use the network to predict the next action to take, using the model\n",
    "        prediction = self.Actor.predict(state)[0]\n",
    "        action = np.random.choice(self.action_size, p=prediction)\n",
    "        action_onehot = np.zeros([self.action_size])\n",
    "        action_onehot[action] = 1\n",
    "        return action, action_onehot, prediction\n",
    "\n",
    "    def discount_rewards(self, reward):#gaes is better\n",
    "        # Compute the gamma-discounted rewards over an episode\n",
    "        # We apply the discount and normalize it to avoid big variability of rewards\n",
    "        gamma = 0.99    # discount rate\n",
    "        running_add = 0\n",
    "        discounted_r = np.zeros_like(reward)\n",
    "        for i in reversed(range(0,len(reward))):\n",
    "            running_add = running_add * gamma + reward[i]\n",
    "            discounted_r[i] = running_add\n",
    "\n",
    "        discounted_r -= np.mean(discounted_r) # normalizing the result\n",
    "        discounted_r /= (np.std(discounted_r) + 1e-8) # divide by standard deviation\n",
    "        return discounted_r\n",
    "\n",
    "    def get_gaes(self, rewards, dones, values, next_values, gamma = 0.99, lamda = 0.9, normalize=True):\n",
    "        deltas = [r + gamma * (1 - d) * nv - v for r, d, nv, v in zip(rewards, dones, next_values, values)]\n",
    "        deltas = np.stack(deltas)\n",
    "        gaes = copy.deepcopy(deltas)\n",
    "        for t in reversed(range(len(deltas) - 1)):\n",
    "            gaes[t] = gaes[t] + (1 - dones[t]) * gamma * lamda * gaes[t + 1]\n",
    "\n",
    "        target = gaes + values\n",
    "        if normalize:\n",
    "            gaes = (gaes - gaes.mean()) / (gaes.std() + 1e-8)\n",
    "        return np.vstack(gaes), np.vstack(target)\n",
    "\n",
    "    def replay(self, states, actions, rewards, predictions, dones, next_states):\n",
    "        # reshape memory to appropriate shape for training\n",
    "        states = np.vstack(states)\n",
    "        next_states = np.vstack(next_states)\n",
    "        actions = np.vstack(actions)\n",
    "        predictions = np.vstack(predictions)\n",
    "\n",
    "        # Get Critic network predictions \n",
    "        values = self.Critic.predict(states)\n",
    "        next_values = self.Critic.predict(next_states)\n",
    "\n",
    "        # Compute discounted rewards and advantages\n",
    "        #discounted_r = self.discount_rewards(rewards)\n",
    "        #advantages = np.vstack(discounted_r - values)\n",
    "        advantages, target = self.get_gaes(rewards, dones, np.squeeze(values), np.squeeze(next_values))\n",
    "        '''\n",
    "        pylab.plot(advantages,'.')\n",
    "        pylab.plot(target,'-')\n",
    "        ax=pylab.gca()\n",
    "        ax.grid(True)\n",
    "        pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "        pylab.show()\n",
    "        '''\n",
    "        # stack everything to numpy array\n",
    "        # pack all advantages, predictions and actions to y_true and when they are received\n",
    "        # in custom PPO loss function we unpack it\n",
    "        y_true = np.hstack([advantages, predictions, actions])\n",
    "        \n",
    "        # training Actor and Critic networks\n",
    "        a_loss = self.Actor.Actor.fit(states, y_true, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "        c_loss = self.Critic.Critic.fit([states, values], target, epochs=self.epochs, verbose=0, shuffle=self.shuffle)\n",
    "\n",
    "        self.writer.add_scalar('Data/actor_loss_per_replay', np.sum(a_loss.history['loss']), self.replay_count)\n",
    "        self.writer.add_scalar('Data/critic_loss_per_replay', np.sum(c_loss.history['loss']), self.replay_count)\n",
    "        self.replay_count += 1\n",
    " \n",
    "    def load(self):\n",
    "        self.Actor.Actor.load_weights(self.Actor_name)\n",
    "        self.Critic.Critic.load_weights(self.Critic_name)\n",
    "\n",
    "    def save(self):\n",
    "        self.Actor.Actor.save_weights(self.Actor_name)\n",
    "        self.Critic.Critic.save_weights(self.Critic_name)\n",
    "        \n",
    "    pylab.figure(figsize=(18, 9))\n",
    "    pylab.subplots_adjust(left=0.05, right=0.98, top=0.96, bottom=0.06)\n",
    "    \n",
    "    def PlotModel(self, score, episode):\n",
    "        self.scores_.append(score)\n",
    "        self.episodes_.append(episode)\n",
    "        self.average_.append(sum(self.scores_[-50:]) / len(self.scores_[-50:]))\n",
    "        if str(episode)[-2:] == \"00\":# much faster than episode % 100\n",
    "            pylab.plot(self.episodes_, self.scores_, 'b')\n",
    "            pylab.plot(self.episodes_, self.average_, 'r')\n",
    "            pylab.title(self.env_name+\" PPO training cycle\", fontsize=18)\n",
    "            pylab.ylabel('Score', fontsize=18)\n",
    "            pylab.xlabel('Steps', fontsize=18)\n",
    "            try:\n",
    "                pylab.grid(True)\n",
    "                pylab.savefig(self.env_name+\".png\")\n",
    "            except OSError:\n",
    "                pass\n",
    "        # saving best models\n",
    "        if self.average_[-1] >= self.max_average:\n",
    "            self.max_average = self.average_[-1]\n",
    "            self.save()\n",
    "            SAVING = \"SAVING\"\n",
    "            # decreaate learning rate every saved model\n",
    "            self.lr *= 0.95\n",
    "            backend.set_value(self.Actor.Actor.optimizer.learning_rate, self.lr)\n",
    "            backend.set_value(self.Critic.Critic.optimizer.learning_rate, self.lr)\n",
    "        else:\n",
    "            SAVING = \"\"\n",
    "\n",
    "        return self.average_[-1], SAVING\n",
    "    \n",
    "    def run(self): # train only when episode is finished\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size[0]])\n",
    "        done, score, SAVING = False, 0, ''\n",
    "        while True:\n",
    "            # Instantiate or reset games memory\n",
    "            states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
    "            while not done:\n",
    "#                 self.env.render()\n",
    "                # Actor picks an action\n",
    "                action, action_onehot, prediction = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                states.append(state)\n",
    "                next_states.append(np.reshape(next_state, [1, self.state_size[0]]))\n",
    "                actions.append(action_onehot)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                predictions.append(prediction)\n",
    "                # Update current state\n",
    "                state = np.reshape(next_state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    self.episode += 1\n",
    "                    average, SAVING = self.PlotModel(score, self.episode)\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "                    \n",
    "                    self.replay(states, actions, rewards, predictions, dones, next_states)\n",
    "\n",
    "                    state, done, score, SAVING = self.env.reset(), False, 0, ''\n",
    "                    state = np.reshape(state, [1, self.state_size[0]])\n",
    "\n",
    "            if self.episode >= self.EPISODES:\n",
    "                break\n",
    "        self.env.close()\n",
    "\n",
    "    def run_batch(self): # train every self.Training_batch episodes\n",
    "        state = self.env.reset()\n",
    "        state = np.reshape(state, [1, self.state_size[0]])\n",
    "        done, score, SAVING = False, 0, ''\n",
    "        while True:\n",
    "            # Instantiate or reset games memory\n",
    "            states, next_states, actions, rewards, predictions, dones = [], [], [], [], [], []\n",
    "            for t in range(self.Training_batch):\n",
    "#                 self.env.render()\n",
    "                # Actor picks an action\n",
    "                action, action_onehot, prediction = self.act(state)\n",
    "                # Retrieve new state, reward, and whether the state is terminal\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # Memorize (state, action, reward) for training\n",
    "                states.append(state)\n",
    "                next_states.append(np.reshape(next_state, [1, self.state_size[0]]))\n",
    "                actions.append(action_onehot)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                predictions.append(prediction)\n",
    "                # Update current state\n",
    "                state = np.reshape(next_state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    self.episode += 1\n",
    "                    average, SAVING = self.PlotModel(score, self.episode)\n",
    "                    print(\"episode: {}/{}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, score, average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{1}/score_per_episode', score, self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{1}/learning_rate', self.lr, self.episode)\n",
    "\n",
    "                    state, done, score, SAVING = self.env.reset(), False, 0, ''\n",
    "                    state = np.reshape(state, [1, self.state_size[0]])\n",
    "                    \n",
    "            self.replay(states, actions, rewards, predictions, dones, next_states)\n",
    "            if self.episode >= self.EPISODES:\n",
    "                break\n",
    "        self.env.close()  \n",
    "\n",
    "        \n",
    "    def run_multiprocesses(self, num_worker = 4):\n",
    "        works, parent_conns, child_conns = [], [], []\n",
    "        for idx in range(num_worker):\n",
    "            parent_conn, child_conn = Pipe()\n",
    "            work = Environment(idx, child_conn, self.env_name, self.state_size[0], self.action_size, False)\n",
    "            work.start()\n",
    "            works.append(work)\n",
    "            parent_conns.append(parent_conn)\n",
    "            child_conns.append(child_conn)\n",
    "\n",
    "        states =        [[] for _ in range(num_worker)]\n",
    "        next_states =   [[] for _ in range(num_worker)]\n",
    "        actions =       [[] for _ in range(num_worker)]\n",
    "        rewards =       [[] for _ in range(num_worker)]\n",
    "        dones =         [[] for _ in range(num_worker)]\n",
    "        predictions =   [[] for _ in range(num_worker)]\n",
    "        score =         [0 for _ in range(num_worker)]\n",
    "\n",
    "        state = [0 for _ in range(num_worker)]\n",
    "        for worker_id, parent_conn in enumerate(parent_conns):\n",
    "            state[worker_id] = parent_conn.recv()\n",
    "\n",
    "        while self.episode < self.EPISODES:\n",
    "            predictions_list = self.Actor.predict(np.reshape(state, [num_worker, self.state_size[0]]))\n",
    "            actions_list = [np.random.choice(self.action_size, p=i) for i in predictions_list]\n",
    "\n",
    "            for worker_id, parent_conn in enumerate(parent_conns):\n",
    "                parent_conn.send(actions_list[worker_id])\n",
    "                action_onehot = np.zeros([self.action_size])\n",
    "                action_onehot[actions_list[worker_id]] = 1\n",
    "                actions[worker_id].append(action_onehot)\n",
    "                predictions[worker_id].append(predictions_list[worker_id])\n",
    "\n",
    "            for worker_id, parent_conn in enumerate(parent_conns):\n",
    "                next_state, reward, done, _ = parent_conn.recv()\n",
    "\n",
    "                states[worker_id].append(state[worker_id])\n",
    "                next_states[worker_id].append(next_state)\n",
    "                rewards[worker_id].append(reward)\n",
    "                dones[worker_id].append(done)\n",
    "                state[worker_id] = next_state\n",
    "                score[worker_id] += reward\n",
    "\n",
    "                if done:\n",
    "                    average, SAVING = self.PlotModel(score[worker_id], self.episode)\n",
    "                    print(\"episode: {}/{}, worker: {}, score: {}, average: {:.2f} {}\".format(self.episode, self.EPISODES, worker_id, score[worker_id], average, SAVING))\n",
    "                    self.writer.add_scalar(f'Workers:{num_worker}/score_per_episode', score[worker_id], self.episode)\n",
    "                    self.writer.add_scalar(f'Workers:{num_worker}/learning_rate', self.lr, self.episode)\n",
    "                    score[worker_id] = 0\n",
    "                    if(self.episode < self.EPISODES):\n",
    "                        self.episode += 1\n",
    "                        \n",
    "            for worker_id in range(num_worker):\n",
    "                if len(states[worker_id]) >= self.Training_batch:\n",
    "                    self.replay(states[worker_id], actions[worker_id], rewards[worker_id], predictions[worker_id], dones[worker_id], next_states[worker_id])\n",
    "                    \n",
    "                    states[worker_id] = []\n",
    "                    next_states[worker_id] = []\n",
    "                    actions[worker_id] = []\n",
    "                    rewards[worker_id] = []\n",
    "                    dones[worker_id] = []\n",
    "                    predictions[worker_id] = []\n",
    "\n",
    "        # terminating processes after while loop\n",
    "        works.append(work)\n",
    "        for work in works:\n",
    "            work.terminate()\n",
    "            print('TERMINATED:', work)\n",
    "            work.join()\n",
    "            \n",
    "\n",
    "    def test(self, test_episodes = 100):\n",
    "        self.load()\n",
    "        for e in range(100):\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, [1, self.state_size[0]])\n",
    "            done = False\n",
    "            score = 0\n",
    "            while not done:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.Actor.predict(state)[0])\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                state = np.reshape(state, [1, self.state_size[0]])\n",
    "                score += reward\n",
    "                if done:\n",
    "                    print(\"episode: {}/{}, score: {}\".format(e, test_episodes, score))\n",
    "                    break\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156750fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattiacinelli/repositories/AlgoRL/.valgorl/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 16:32:31.515364: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-21 16:32:31.542240: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fc9893520d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-03-21 16:32:31.542270: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/100, score: -182.16741955202667, average: -182.17 \n",
      "episode: 2/100, score: -443.26362134065255, average: -312.72 \n",
      "episode: 3/100, score: -144.21004542806807, average: -256.55 \n",
      "episode: 4/100, score: -114.9286060255158, average: -221.14 \n",
      "episode: 5/100, score: -113.23240801493205, average: -199.56 \n",
      "episode: 6/100, score: -232.80321635518146, average: -205.10 \n",
      "episode: 7/100, score: -105.56557828126434, average: -190.88 \n",
      "episode: 8/100, score: -323.18151130360695, average: -207.42 \n",
      "episode: 9/100, score: -120.80619632553001, average: -197.80 \n",
      "episode: 10/100, score: -5.5730202877115715, average: -178.57 \n",
      "episode: 11/100, score: -123.11238016427494, average: -173.53 \n",
      "episode: 12/100, score: -132.04349235696816, average: -170.07 \n",
      "episode: 13/100, score: -93.43720615032399, average: -164.18 \n",
      "episode: 14/100, score: -159.8532756462103, average: -163.87 \n",
      "episode: 15/100, score: -51.850137386476376, average: -156.40 \n",
      "episode: 16/100, score: -134.28366024856405, average: -155.02 \n",
      "episode: 17/100, score: -183.4783414727071, average: -156.69 \n",
      "episode: 18/100, score: -102.68738771647023, average: -153.69 \n",
      "episode: 19/100, score: -314.1948830572013, average: -162.14 \n",
      "episode: 20/100, score: -178.6859168524541, average: -162.97 \n",
      "episode: 21/100, score: -94.61334188429231, average: -159.71 \n",
      "episode: 22/100, score: -77.11353599583526, average: -155.96 \n",
      "episode: 23/100, score: -62.13705538848657, average: -151.88 \n",
      "episode: 24/100, score: -106.48007152078172, average: -149.99 \n",
      "episode: 25/100, score: -152.67144499201004, average: -150.09 \n",
      "episode: 26/100, score: -96.18055000811536, average: -148.02 \n",
      "episode: 27/100, score: -95.6359446280538, average: -146.08 \n",
      "episode: 28/100, score: -47.5418867953274, average: -142.56 \n",
      "episode: 29/100, score: -82.85952594332251, average: -140.50 \n",
      "episode: 30/100, score: -100.97608184378917, average: -139.19 \n",
      "episode: 31/100, score: -85.54043500765054, average: -137.46 \n",
      "episode: 32/100, score: -94.55583087285137, average: -136.11 \n",
      "episode: 33/100, score: -199.59550173033813, average: -138.04 \n",
      "episode: 34/100, score: -211.53140716973803, average: -140.20 \n",
      "episode: 35/100, score: -298.0466109970516, average: -144.71 \n",
      "episode: 36/100, score: -143.5880987563313, average: -144.68 \n",
      "episode: 37/100, score: -65.15220903845832, average: -142.53 \n",
      "episode: 38/100, score: -135.8836884572098, average: -142.35 \n",
      "episode: 39/100, score: -93.01366115388976, average: -141.09 \n",
      "episode: 40/100, score: -44.167888178446574, average: -138.67 \n",
      "episode: 41/100, score: -52.58761075426713, average: -136.57 \n",
      "episode: 42/100, score: -61.21625233575263, average: -134.77 \n",
      "episode: 43/100, score: -173.3468747346041, average: -135.67 \n",
      "episode: 44/100, score: 18.788330848727952, average: -132.16 \n",
      "episode: 45/100, score: 13.909573841007003, average: -128.91 \n",
      "episode: 46/100, score: -106.32650070367418, average: -128.42 \n",
      "episode: 47/100, score: -30.728479057685604, average: -126.34 \n",
      "episode: 48/100, score: -60.56388252663749, average: -124.97 \n",
      "episode: 49/100, score: -101.05420028358223, average: -124.49 \n",
      "episode: 50/100, score: -51.02903324722196, average: -123.02 \n",
      "episode: 51/100, score: -133.13996162120966, average: -122.04 \n",
      "episode: 52/100, score: -25.88357360500963, average: -113.69 \n",
      "episode: 53/100, score: -69.96152714079788, average: -112.20 \n",
      "episode: 54/100, score: 17.968392827853492, average: -109.54 \n",
      "episode: 55/100, score: -104.47771803933784, average: -109.37 \n",
      "episode: 56/100, score: -84.89993854887372, average: -106.41 \n",
      "episode: 57/100, score: 21.291757679119954, average: -103.87 \n",
      "episode: 58/100, score: 145.36263459183277, average: -94.50 \n",
      "episode: 59/100, score: -98.75138156467676, average: -94.06 \n",
      "episode: 60/100, score: -28.450332415928216, average: -94.52 \n",
      "episode: 61/100, score: -16.68365823679926, average: -92.39 \n",
      "episode: 62/100, score: -183.53675726610516, average: -93.42 \n",
      "episode: 63/100, score: -58.823014511307846, average: -92.73 \n",
      "episode: 64/100, score: -155.71253059912058, average: -92.65 \n",
      "episode: 65/100, score: -176.72033873404672, average: -95.14 \n",
      "episode: 66/100, score: 35.750363615588796, average: -91.74 \n",
      "episode: 67/100, score: 16.131105409590926, average: -87.75 \n",
      "episode: 68/100, score: -112.82852462701844, average: -87.95 \n",
      "episode: 69/100, score: -66.78994157340024, average: -83.01 \n",
      "episode: 70/100, score: -47.789044898727155, average: -80.39 \n",
      "episode: 71/100, score: -38.753722639132164, average: -79.27 \n",
      "episode: 72/100, score: -29.948668166020468, average: -78.33 \n",
      "episode: 73/100, score: -93.10415406741608, average: -78.95 \n",
      "episode: 74/100, score: -5.244147387422984, average: -76.92 \n",
      "episode: 75/100, score: -102.13393483639193, average: -75.91 \n",
      "episode: 76/100, score: 11.954396394387999, average: -73.75 \n",
      "episode: 77/100, score: -47.337022383984035, average: -72.78 \n",
      "episode: 78/100, score: -224.4997946379039, average: -76.32 \n",
      "episode: 79/100, score: -178.6085790451849, average: -78.24 \n",
      "episode: 80/100, score: -73.29615380326582, average: -77.68 \n",
      "episode: 81/100, score: -22.14624267771731, average: -76.42 \n",
      "episode: 82/100, score: -170.00774400277555, average: -77.92 \n",
      "episode: 83/100, score: 40.229436390550426, average: -73.13 \n",
      "episode: 84/100, score: -87.16113893997974, average: -70.64 \n",
      "episode: 85/100, score: -63.83779293083955, average: -65.96 \n",
      "episode: 86/100, score: -14.766482783508298, average: -63.38 \n",
      "episode: 87/100, score: -167.13103324814466, average: -65.42 \n",
      "episode: 88/100, score: -71.6046200138999, average: -64.13 \n",
      "episode: 89/100, score: -20.650001310272987, average: -62.69 \n",
      "episode: 90/100, score: 93.06903227448275, average: -59.94 \n",
      "episode: 91/100, score: -208.89282485172248, average: -63.07 \n",
      "episode: 92/100, score: -94.86014335130467, average: -63.74 \n",
      "episode: 93/100, score: -266.71157521289064, average: -65.61 \n",
      "episode: 94/100, score: -203.06625646553036, average: -70.04 \n",
      "episode: 95/100, score: -49.00404045159811, average: -71.30 \n",
      "episode: 96/100, score: -98.42804996843522, average: -71.15 \n",
      "episode: 97/100, score: 3.1718282660208956, average: -70.47 \n",
      "episode: 98/100, score: 17.065680137631574, average: -68.91 \n",
      "episode: 99/100, score: -39.54464832060259, average: -67.68 \n",
      "episode: 100/100, score: -10.275966427799437, average: -66.87 \n",
      "episode: 101/100, score: -115.45798486088285, average: -66.52 \n",
      "episode: 102/100, score: -86.08255724051355, average: -67.72 \n",
      "episode: 103/100, score: 55.088590394524616, average: -65.22 \n",
      "episode: 104/100, score: -169.65647960771258, average: -68.97 \n",
      "episode: 105/100, score: -68.83176662974324, average: -68.26 \n",
      "episode: 106/100, score: -225.51280387664164, average: -71.07 \n",
      "episode: 107/100, score: -582.7425492152345, average: -83.15 \n",
      "episode: 108/100, score: -28.829350837074614, average: -86.64 \n",
      "episode: 109/100, score: -109.43845981529327, average: -86.85 \n",
      "episode: 110/100, score: -116.01175531381165, average: -88.60 \n",
      "episode: 111/100, score: -117.60114440384596, average: -90.62 \n",
      "episode: 112/100, score: -127.0308231064972, average: -89.49 \n",
      "episode: 113/100, score: -99.80207834715257, average: -90.31 \n",
      "episode: 114/100, score: -177.79295162355834, average: -90.75 \n",
      "episode: 115/100, score: -234.28298115682537, average: -91.90 \n",
      "episode: 116/100, score: -65.77415690595501, average: -93.93 \n",
      "episode: 117/100, score: -8.250113194155745, average: -94.42 \n",
      "episode: 118/100, score: -134.0282827739377, average: -94.84 \n",
      "episode: 119/100, score: -160.25630165953015, average: -96.71 \n",
      "episode: 120/100, score: -137.21067321046502, average: -98.50 \n",
      "episode: 121/100, score: -159.33377331129265, average: -100.91 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m env_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLunarLander-v2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m agent \u001b[38;5;241m=\u001b[39m PPOAgent(env_name)\n\u001b[0;32m----> 4\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# train as PPO, train every epesode\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#agent.run_batch() # train as PPO, train every batch, trains better\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#agent.run_multiprocesses(num_worker = 8)  # train PPO multiprocessed (fastest)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m agent\u001b[38;5;241m.\u001b[39mtest()\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mPPOAgent.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorkers:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/score_per_episode\u001b[39m\u001b[38;5;124m'\u001b[39m, score, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWorkers:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/learning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode)\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m state, done, score, SAVING \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(), \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    183\u001b[0m state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(state, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_size[\u001b[38;5;241m0\u001b[39m]])\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mPPOAgent.replay\u001b[0;34m(self, states, actions, rewards, predictions, dones, next_states)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# training Actor and Critic networks\u001b[39;00m\n\u001b[1;32m    103\u001b[0m a_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mActor\u001b[38;5;241m.\u001b[39mActor\u001b[38;5;241m.\u001b[39mfit(states, y_true, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle)\n\u001b[0;32m--> 104\u001b[0m c_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCritic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/actor_loss_per_replay\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(a_loss\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_count)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/critic_loss_per_replay\u001b[39m\u001b[38;5;124m'\u001b[39m, np\u001b[38;5;241m.\u001b[39msum(c_loss\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_count)\n",
      "File \u001b[0;32m~/repositories/AlgoRL/.valgorl/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py:790\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    789\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/AlgoRL/.valgorl/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_arrays.py:649\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`validation_steps` should not be specified if \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    646\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`validation_data` is None.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    647\u001b[0m   val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_sample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/AlgoRL/.valgorl/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_arrays.py:386\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m callbacks\u001b[38;5;241m.\u001b[39m_call_batch_hook(mode, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_index, batch_logs)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;66;03m# Get outputs.\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m batch_outs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mins_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_outs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    388\u001b[0m   batch_outs \u001b[38;5;241m=\u001b[39m [batch_outs]\n",
      "File \u001b[0;32m~/repositories/AlgoRL/.valgorl/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:3790\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m   3788\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mflatten(inputs, expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 3790\u001b[0m   session \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m   feed_arrays \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3792\u001b[0m   array_vals \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/repositories/AlgoRL/.valgorl/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:630\u001b[0m, in \u001b[0;36mget_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _MANUAL_VAR_INIT:\n\u001b[1;32m    629\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[0;32m--> 630\u001b[0m     \u001b[43m_initialize_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/repositories/AlgoRL/.valgorl/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:1046\u001b[0m, in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m   1044\u001b[0m variables \u001b[38;5;241m=\u001b[39m _get_variables(get_graph())\n\u001b[1;32m   1045\u001b[0m candidate_vars \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1046\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m variables:\n\u001b[1;32m   1047\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(v, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_initialized\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1048\u001b[0m     candidate_vars\u001b[38;5;241m.\u001b[39mappend(v)\n",
      "File \u001b[0;32m~/repositories/AlgoRL/.valgorl/lib/python3.8/site-packages/tensorflow/python/util/object_identity.py:230\u001b[0m, in \u001b[0;36mObjectIdentityWeakSet.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[0;32m--> 230\u001b[0m   unwrapped \u001b[38;5;241m=\u001b[39m \u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrapped\u001b[49m\n\u001b[1;32m    231\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m unwrapped \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m~/repositories/AlgoRL/.valgorl/lib/python3.8/site-packages/tensorflow/python/util/object_identity.py:80\u001b[0m, in \u001b[0;36m_WeakObjectIdentityWrapper.unwrapped\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 80\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "env_name = 'LunarLander-v2'\n",
    "agent = PPOAgent(env_name)\n",
    "agent.run() # train as PPO, train every epesode\n",
    "#agent.run_batch() # train as PPO, train every batch, trains better\n",
    "#agent.run_multiprocesses(num_worker = 8)  # train PPO multiprocessed (fastest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "664ec36f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'LunarLander-v2_PPO_Actor.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mPPOAgent.test\u001b[0;34m(self, test_episodes)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(\u001b[38;5;28mself\u001b[39m, test_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m    302\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mPPOAgent.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mActor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mActor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mActor_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCritic\u001b[38;5;241m.\u001b[39mCritic\u001b[38;5;241m.\u001b[39mload_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCritic_name)\n",
      "File \u001b[0;32m~/repositories/AlgoRL/.valgorl/lib/python3.8/site-packages/tensorflow/python/keras/engine/training_v1.py:238\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy\u001b[38;5;241m.\u001b[39mextended\u001b[38;5;241m.\u001b[39msteps_per_run \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    235\u001b[0m       (\u001b[38;5;129;01mnot\u001b[39;00m training_lib\u001b[38;5;241m.\u001b[39m_is_hdf5_filepath(filepath))):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoad weights is not yet supported with TPUStrategy \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    237\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwith steps_per_run greater than 1.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_mismatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repositories/AlgoRL/.valgorl/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:2204\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2199\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2200\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnable to load weights saved in HDF5 format into a subclassed \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2201\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel which has not created its variables yet. Call the Model \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2202\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst, then load the weights.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_weights_created()\n\u001b[0;32m-> 2204\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mh5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   2205\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_names\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mattrs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[1;32m   2206\u001b[0m     f \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/repositories/AlgoRL/.valgorl/lib/python3.8/site-packages/h5py/_hl/files.py:406\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m phil:\n\u001b[1;32m    405\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m--> 406\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_fcpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrack_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack_order\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/repositories/AlgoRL/.valgorl/lib/python3.8/site-packages/h5py/_hl/files.py:173\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    172\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 173\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    175\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:88\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'LunarLander-v2_PPO_Actor.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8f44f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".valgorl_nb",
   "language": "python",
   "name": ".valgorl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
